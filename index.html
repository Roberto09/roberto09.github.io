<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>Roberto Garcia</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <link rel="stylesheet" href="style.css" />
</head>

<body>
  <main>
    <header class="header">
      <div class="header-text">
        <h1>Roberto Garcia</h1>
        <p class="subtitle">
          Research Assistant @ Hazy Research, Stanford<br />
          <a href="mailto:roberto.gt1509@gmail.com">roberto.gt1509@gmail.com</a>
        </p>
      </div>
    
      <div class="header-photo">
        <img src="./IMG_0968_roberto.jpg" alt="Roberto Garcia" />
      </div>
    </header>    

    <section>
      <h2>About</h2>
      <p>
        I am broadly interested in developing machine learning systems from
        mathematical first principles, with a focus on understanding and
        constructing neural architectures with well-defined behaviors.
        While large foundation models have demonstrated remarkable empirical
        success, many core questions remain about why modern architectures work
        and how to design them in a principled way.
      </p>
      <p>
        My recent work explores how structural and geometric insights can be used
        to both improve efficiency and reveal fundamental limits of existing
        approaches. This includes designing adaptive low-rank mechanisms for
        transformer inference and constructing multilayer perceptrons in closed
        form to store factual knowledge at information-theoretically optimal
        rates. I am particularly motivated by approaches that move beyond
        empirical trial-and-error toward architectures whose capabilities can be
        explicitly built, analyzed, and edited.
      </p>
    </section>

    <section>
      <h2>Research Interests</h2>
      <ul>
        <li>Mathematically grounded neural architectures</li>
        <li>Training dynamics and optimization of large-scale models</li>
        <li>Efficient inference and adaptive computation in transformers</li>
        <li>Low-rank structure, sparsity, and model compression</li>
        <li>Constructive approaches to representation learning and factual memory</li>
      </ul>
    </section>

    <section>
      <h2>Selected Publications</h2>
    
      <div class="pub">
        <p class="title">
          Constructing Efficient Fact-Storing MLPs for Transformers
        </p>
        <p class="authors">
          Owen Dugan*, <span class="me">Roberto Garcia*</span>, Ronny Junkins*, Jerry Liu*, Dylan Zinsley,
          Sabri Eyuboglu, Atri Rudra, and Chris Ré
        </p>
        <p>
          <a href="https://arxiv.org/abs/2512.00207" target="_blank">paper</a>
        </p>
      </div>
    
      <div class="pub">
        <p class="title">
          Adaptive Rank Allocation: Speeding Up Modern Transformers with RaNA Adapters
        </p>
        <p class="authors">
          <span class="me">Roberto Garcia</span>, Jerry Liu, Daniel Sorvisto, and Sabri Eyuboglu
        </p>
        <p>
          <a href="https://proceedings.iclr.cc/paper_files/paper/2025/file/bdb0596d13cfccf2db6f0cc5280d2a3f-Paper-Conference.pdf"
             target="_blank">
            paper
          </a>
          &nbsp;·&nbsp; ICLR 2025
        </p>
      </div>
    
      <div class="pub">
        <p class="title">
          Combining Constructive and Perturbative Deep Learning Algorithms for the Capacitated Vehicle Routing Problem
        </p>
        <p class="authors">
          <span class="me">Roberto García-Torres</span>, Alitzel Adriana Macias-Infante,
          Santiago Enrique Conant-Pablos, José Carlos Ortiz-Bayliss,
          and Hugo Terashima-Marín
        </p>
        <p>
          <a href="https://arxiv.org/abs/2211.13922" target="_blank">paper</a>
        </p>
      </div>
    </section>
    
    <footer>
      <p>
        © 2025 Roberto Garcia
      </p>
    </footer>
  </main>
</body>
</html>
